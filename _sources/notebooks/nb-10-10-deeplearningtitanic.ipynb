{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningTitanic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRlGzpOI8eO0"
      },
      "source": [
        "\n",
        "[![AnalyticsDojo](https://github.com/rpi-techfundamentals/spring2019-materials/blob/master/fig/final-logo.png?raw=1)](http://rpi.analyticsdojo.com)\n",
        "<center><h1>Titanic Classification - Keras API</h1></center>\n",
        "<center><h3><a href = 'http://introml.analyticsdojo.com'>introml.analyticsdojo.com</a></h3></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XovA71E3XFM"
      },
      "source": [
        "# Titanic Classification - Deep Learning Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pW1UhJT8ePk"
      },
      "source": [
        "As an example of how to work with both categorical and numerical data, we will perform survival predicition for the passengers of the HMS Titanic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvj3Wids8ePm"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/rpi-techfundamentals/spring2019-materials/master/input/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/rpi-techfundamentals/spring2019-materials/master/input/test.csv')\n",
        "\n",
        "print(train.columns, test.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xqjk2-P8ePp"
      },
      "source": [
        "Here is a broad description of the keys and what they mean:\n",
        "\n",
        "```\n",
        "pclass          Passenger Class\n",
        "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "survival        Survival\n",
        "                (0 = No; 1 = Yes)\n",
        "name            Name\n",
        "sex             Sex\n",
        "age             Age\n",
        "sibsp           Number of Siblings/Spouses Aboard\n",
        "parch           Number of Parents/Children Aboard\n",
        "ticket          Ticket Number\n",
        "fare            Passenger Fare\n",
        "cabin           Cabin\n",
        "embarked        Port of Embarkation\n",
        "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
        "boat            Lifeboat\n",
        "body            Body Identification Number\n",
        "home.dest       Home/Destination\n",
        "```\n",
        "\n",
        "In general, it looks like `name`, `sex`, `cabin`, `embarked`, `boat`, `body`, and `homedest` may be candidates for categorical features, while the rest appear to be numerical features. We can also look at the first couple of rows in the dataset to get a better understanding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmMR9G78ePr"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54WY6zD78ePv"
      },
      "source": [
        "### Preprocessing function\n",
        "\n",
        "We want to create a preprocessing function that can address transformation of our train and test set.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKX26KU34Ti6"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "cat_features = ['Pclass', 'Sex', 'Embarked']\n",
        "num_features =  [ 'Age', 'SibSp', 'Parch', 'Fare'  ]\n",
        "\n",
        "\n",
        "def preprocess(df, num_features, cat_features, dv):\n",
        "    features = cat_features + num_features\n",
        "    if dv in df.columns:\n",
        "      y = df[dv]\n",
        "    else:\n",
        "      y=None \n",
        "    #Address missing variables\n",
        "    print(\"Total missing values before processing:\", df[features].isna().sum().sum() )\n",
        "  \n",
        "    imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "    df[cat_features]=imp_mode.fit_transform(df[cat_features] )\n",
        "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    df[num_features]=imp_mean.fit_transform(df[num_features])\n",
        "    print(\"Total missing values after processing:\", df[features].isna().sum().sum() )\n",
        "   \n",
        "    X = pd.get_dummies(df[features], columns=cat_features, drop_first=True)\n",
        "    return y,X\n",
        "\n",
        "y, X =  preprocess(train, num_features, cat_features, 'Survived')\n",
        "test_y, test_X = preprocess(test, num_features, cat_features, 'Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV5s-bSMJPne"
      },
      "source": [
        "### Train Test Split\n",
        "\n",
        "Now we are ready to model. We are going to separate our Kaggle given data into a \"Train\" and a \"Validation\" set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icKFkwZQpvCs"
      },
      "source": [
        "#Import Module\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=122,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfJues_6jaXA"
      },
      "source": [
        "train_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG9S2aUvh2Cl"
      },
      "source": [
        "### Sequential Model Classification. \n",
        "\n",
        "This is our training. We do all of the preprocessing our old way and just use the dataframe.values to pass to Keras.\n",
        "\n",
        "https://keras.io/guides/sequential_model/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAUV7oYp7HZV"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import metrics\n",
        "\n",
        "\n",
        "#Create our model using sequential mode\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=9, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uiIwmm2kJ8e"
      },
      "source": [
        "#Specify the model \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Fit the model\n",
        "model.fit(train_X.values, train_y.values, epochs=100, batch_size=20, verbose=2)\n",
        "\n",
        "_, trainperf = model.evaluate(train_X, train_y)\n",
        "_, testperf = model.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGoUxc7brPIg"
      },
      "source": [
        "# Alternate Sequential syntax\n",
        "import tensorflow as tf\n",
        "altmodel = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(20, input_dim=9, activation='relu'),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "altmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "altmodel.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChXuWRtpqMhy"
      },
      "source": [
        "\n",
        "#Specify the model \n",
        "#Fit the model\n",
        "altmodel.fit(train_X.values, train_y.values, epochs=100, batch_size=20, verbose=2)\n",
        "\n",
        "_, altmodelTrainperf = altmodel.evaluate(train_X, train_y)\n",
        "_, altmodelValPerf = altmodel.evaluate(val_X, val_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s16HjNlF--Th"
      },
      "source": [
        "## Functional Model \n",
        "\n",
        "https://keras.io/guides/functional_api/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz7_nsvP3XEI"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(9,))\n",
        "x = tf.keras.layers.Dense(20, activation=tf.nn.relu)(inputs)\n",
        "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(x)\n",
        "outputs = tf.keras.layers.Dense(1)(x)\n",
        "modelalt2 = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
        "modelalt2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WGgsasSf80U"
      },
      "source": [
        "# The Keras Model Subclassing Methods.\n",
        "\n",
        "https://keras.io/api/models/model/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wPaXBZOfg9U"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(20, input_dim=9, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(100, activation=tf.nn.relu)\n",
        "    self.dense3 = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    return self.dense3(x)\n",
        "\n",
        "altmodel3 = MyModel()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O10sGaYHD_n0"
      },
      "source": [
        "altmodel3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#Fit the model\n",
        "altmodel3.fit(train_X.values, train_y.values, epochs=100, batch_size=20, verbose=2)\n",
        "\n",
        "altmodel3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4onf6A9A3YCZ"
      },
      "source": [
        ""
      ]
    }
  ]
}